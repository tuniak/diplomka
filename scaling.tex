\chapter{Parallel scaling performace of PI automata}
In this chapter, we provide a useful measure of our parallel implementation, the \textit{scaling efficiency}. It indicates the speedup of the computation with the increasing number of processing units employed.
We will use two distinct ideas of scaling efficiency.

\section{Strong scaling}
In strong scaling measurement, we vary the number of processing units, while the problem size stays fixed.

The formula is as simple as
\begin{align*}
s_N = \frac{t_1}{N t_N} 100\%,
\end{align*}
where $t_1$ is a time to finish the work with one processing unit and $t_N$ is time to finish the same amount of work with $N$ processing units.

The idea of strong scaling is to find a "sweet spot" -- optimal number of computational units to compensate for the overhead of parrallelisation.
%relative to the overhead ...
\section{Weak scaling}
In this measurement, we vary number of processing units, but we vary also the size of the problem accordingly, so that amount of work per one processing unit is constant.

The weak scaling formula reads
\begin{align*}
w_N = \frac{t_1}{t_N} 100\%,
\end{align*}
where $t_1$ is time to complete $1$ unit of workload with $1$ processing unit, while $t_N$ is time to complete $N$ units of workload with $N$ processing units.

Therefore, the weak scaling tells us how requirements on memory bandwidth and other resources depend on the size of the problem.

\section{Flow on the sphere}
The most time--consuming simulation that we performed was the turbulent flow inside the sphere, so the measurement of scaling on this problem is of the highest importance.

The strong scaling was measured on the sphere with diameter $d = 500$ over $3000$ time-steps.
On the graph below, we plotted time--length of the last $1000$ steps against the number of cores employed.
\begin{figure}[H]
 \centering 
 \includegraphics[width=1\textwidth]{./img/strongscalingg}
\end{figure}

For the weak scaling, the diameter of the sphere varied, so that the volume of lattice was increasing proportionally to the number of threads. However, side of the lattice must be multiple of $10$, because macroscopic velocity is computed over the cube with side of $10$. Therefore, the number of nodes per one thread slightly differs -- by no more then $5\%$ at most.

\begin{tabular}{ |p{2cm}|p{4cm}|p{4cm}| }
 \hline
 Threads & Diameter [nodes] & Nodes per one thread \\
 \hline
 \hline
$1$ & $320$ & $32\,768\,000$ \\
 \hline
$2$ & $400$ & $32\,000\,000$\\
 \hline
$4$ & $500$ & $31\,250\,000$ \\
 \hline
$8$ & $630$ & $31\,255\,875$\\
 \hline
$16$ & $800$ & $32\,000\,000$ \\
 \hline
$32$ & $1000$ & $31\,250\,000$ \\
 \hline
 \end{tabular}


\begin{figure}[H]
 \centering 
 \includegraphics[width=1\textwidth]{./img/weakscaling}
\end{figure}

All computations were performed on the cluster Alfrid of University of West Bohemia.

\begin{tabular}{ |p{4cm}|p{8.8cm}| }
\hline
\multicolumn{2}{|c|}{Alfrid cluster} \\
\hline
\hline
Architecture &          x$86\_64$ \\
\hline
CPU op-mode(s) &        32-bit, 64-bit \\
\hline
Byte Order &            Little Endian \\
\hline
CPU(s) &                32 \\
\hline
On-line CPU(s) list &   0-31 \\
\hline
Thread(s) per core &    2 \\
\hline
Core(s) per socket &    8 \\
\hline
Socket(s) &             2 \\
\hline
NUMA node(s) &          2 \\
\hline
Vendor ID &             GenuineIntel \\
\hline
CPU family &            6 \\
\hline
Model &                 62 \\
\hline
Model name &            Intel(R) Xeon(R) CPU E5-2650 v2 @ 2.60GHz \\
\hline
Stepping &              4 \\
\hline
CPU MHz &               1216.007 \\
\hline
CPU max MHz &           3400.0000 \\
\hline
CPU min MHz &           1200.0000 \\
\hline
BogoMIPS &              5201.43 \\
\hline
Virtualization &        VT-x \\
\hline
L1d cache &            32K \\
\hline
L1i cache &            32K \\
\hline
L2 cache &            256K \\
\hline
L3 cache &              20480K \\
\hline
NUMA node0 CPU(s) &     0-7,16-23 \\
\hline
NUMA node1 CPU(s) &     8-15,24-31 \\
 \hline
 \end{tabular}